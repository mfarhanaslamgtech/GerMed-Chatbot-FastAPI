{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# GerMed ChatBot (Gemedusa) - Embedding Utilities\n",
                "\n",
                "This notebook provides utilities for generating and testing text and image embeddings using the modernized GerMed-Chatbot-FastAPI infrastructure.\n",
                "\n",
                "## üéì Key Features:\n",
                "- Uses the same `TextEmbeddingModel` and `ImageEmbeddingModel` singletons as the FastAPI app.\n",
                "- Directly interacts with the models for vector generation.\n",
                "- Optimized for testing without intermediate file storage."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup Environment"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import sys\n",
                "import torch\n",
                "import numpy as np\n",
                "from pathlib import Path\n",
                "from PIL import Image\n",
                "from io import BytesIO\n",
                "import httpx\n",
                "\n",
                "# Add project root to path for imports\n",
                "project_root = Path(os.getcwd()).parent.parent\n",
                "if str(project_root) not in sys.path:\n",
                "    sys.path.append(str(project_root))\n",
                "\n",
                "from src.app.utils.embedding_model import TextEmbeddingModel, ImageEmbeddingModel\n",
                "from src.app.config.settings import settings\n",
                "\n",
                "print(f\"‚úÖ Environment Ready. Project Root: {project_root}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Text Embedding Laboratory"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load Text Model (SentenceTransformer)\n",
                "text_model = TextEmbeddingModel.get_instance()\n",
                "\n",
                "def get_text_vector(text: str):\n",
                "    return text_model.encode(text).tolist()\n",
                "\n",
                "sample_query = \"Veterinary surgical forceps for small cats\"\n",
                "vector = get_text_vector(sample_query)\n",
                "print(f\"Generated vector for '{sample_query}' (Size: {len(vector)})\")\n",
                "print(f\"Preview (first 5): {vector[:5]}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Vision Embedding Laboratory (CLIP)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load Image Model (CLIP)\n",
                "clip_data = ImageEmbeddingModel.get_instance()\n",
                "model = clip_data[\"model\"]\n",
                "processor = clip_data[\"processor\"]\n",
                "device = clip_data[\"device\"]\n",
                "\n",
                "def get_image_vector(image_input):\n",
                "    if isinstance(image_input, str) and image_input.startswith('http'):\n",
                "        response = httpx.get(image_input)\n",
                "        img = Image.open(BytesIO(response.content))\n",
                "    else:\n",
                "        img = Image.open(image_input)\n",
                "    \n",
                "    if img.mode != 'RGB':\n",
                "        img = img.convert('RGB')\n",
                "        \n",
                "    with torch.no_grad():\n",
                "        inputs = processor(images=img, return_tensors=\"pt\", padding=True)\n",
                "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
                "        outputs = model.get_image_features(**inputs)\n",
                "        \n",
                "        # Handle structured output\n",
                "        if hasattr(outputs, \"image_embeds\"):\n",
                "            features = outputs.image_embeds\n",
                "        elif hasattr(outputs, \"pooler_output\"):\n",
                "            features = outputs.pooler_output\n",
                "        else:\n",
                "            features = outputs\n",
                "            \n",
                "        features = features.cpu().numpy()\n",
                "        norm = np.linalg.norm(features)\n",
                "        features = features / (norm + 1e-12)\n",
                "        \n",
                "    return features.flatten().tolist()\n",
                "\n",
                "sample_image_url = \"https://www.gervetusa.com/up_data/products/images/medium/1-110-14.jpg\"\n",
                "try:\n",
                "    img_vector = get_image_vector(sample_image_url)\n",
                "    print(f\"Generated image vector (Size: {len(img_vector)})\")\n",
                "    print(f\"Preview (first 5): {img_vector[:5]}\")\n",
                "except Exception as e:\n",
                "    print(f\"‚ùå Could not process image: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Similarity Testing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def cosine_similarity(v1, v2):\n",
                "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
                "\n",
                "print(\"Utility loaded: cosine_similarity(v1, v2)\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_light_formatter": "lexers.HtmlFormatter",
            "pygments_style": "long",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}